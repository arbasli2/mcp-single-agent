# Local LLM Configuration (default)
# For local machine: http://localhost:1234/v1
# For WSL accessing Windows LM Studio: use Windows host IP (see below)
LOCAL_LLM_BASE_URL=http://localhost:1234/v1
LOCAL_LLM_API_KEY=lm-studio

# WSL Configuration: 
# If running in WSL and LM Studio is on Windows, find your Windows host IP:
# Run: ip route show | grep default
# Then use: http://YOUR_WINDOWS_IP:1234/v1
# Example: LOCAL_LLM_BASE_URL=http://172.18.224.1:1234/v1

# OpenAI Configuration (optional)
# Set USE_OPENAI=true to use OpenAI instead of local LLM
USE_OPENAI=false
OPENAI_API_KEY=your_openai_api_key_here

# Alternative local LLM endpoints:
# Ollama: http://localhost:11434/v1
# vLLM: http://localhost:8000/v1
# Text Generation WebUI: http://localhost:5000/v1

# YouTube Data API (required for search_youtube_videos tool)
YOUTUBE_API_KEY=your_youtube_api_key_here

# Google Programmable Search Engine (required for search_web tool)
GOOGLE_CSE_API_KEY=your_google_api_key_here
GOOGLE_CSE_ID=your_custom_search_engine_id_here